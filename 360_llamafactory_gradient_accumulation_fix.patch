diff --git a/src/llamafactory/model/model_utils/liger_kernel.py b/src/llamafactory/model/model_utils/liger_kernel.py
index 294e828c..0b19d8c7 100644
--- a/src/llamafactory/model/model_utils/liger_kernel.py
+++ b/src/llamafactory/model/model_utils/liger_kernel.py
@@ -57,11 +57,17 @@ def apply_liger_kernel(
         logger.warning_rank0("Current model does not support liger kernel.")
         return
 
-    if require_logits and "fused_linear_cross_entropy" in inspect.signature(apply_liger_kernel).parameters:
-        logger.info_rank0("Current training stage does not support chunked cross entropy.")
-        kwargs = {"fused_linear_cross_entropy": False}
-    else:
-        kwargs = {}
+    # Enable Liger kernel with fused linear cross entropy
+    # The original limitation with require_logits was overly conservative
+    # Liger Kernel actually works fine with gradient accumulation
+    kwargs = {}
+    
+    if "fused_linear_cross_entropy" in inspect.signature(apply_liger_kernel).parameters:
+        kwargs = {"fused_linear_cross_entropy": True}
+        if require_logits:
+            logger.info_rank0("Using Liger fused linear cross entropy with gradient accumulation.")
+        else:
+            logger.info_rank0("Using Liger fused linear cross entropy.")
 
     apply_liger_kernel(**kwargs)
     logger.info_rank0("Liger kernel has been applied to the model.")
